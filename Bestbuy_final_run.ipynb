{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQo0WGwuIyj7",
        "outputId": "476997a7-a83b-4a47-895b-1ad8e40de6b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIb-RNYuI5P6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "file_path1 = '/content/drive/MyDrive/Bestbuy/training_dataset.xlsx'\n",
        "file_path2 = '/content/drive/MyDrive/Bestbuy/validation_dataset.xlsx'\n",
        "file_path3 = '/content/drive/MyDrive/Bestbuy/test_dataset.xlsx'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hS0bMrm6I5ST"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_excel(file_path1)\n",
        "valid_df = pd.read_excel(file_path2)\n",
        "test_df = pd.read_excel(file_path3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9pKJ2y2JeIU",
        "outputId": "66a451b7-feb2-4704-d47b-97f17e53db71"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "# Download NLTK resources (stopwords and WordNet)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "import spacy\n",
        "\n",
        "\n",
        "# Load the spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIProyfsJeKj",
        "outputId": "204b348f-0c73-4a4f-81a8-97e14337d2c2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from multiprocessing import Pool, cpu_count\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAaoRQXBJeMu"
      },
      "outputs": [],
      "source": [
        "def remove_small_sentences(text):\n",
        "    sentences = text.split('.')\n",
        "    imp_sentences =''\n",
        "    for s in sentences:\n",
        "        words = s.split()\n",
        "        l = len(words)\n",
        "        if l>2: #removing sentences that are very short\n",
        "            s_filtered = POS_filter(s)\n",
        "            imp_sentences += s_filtered+'.'\n",
        "        else:\n",
        "            continue\n",
        "    return imp_sentences\n",
        "\n",
        "def POS_filter(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "# Extract parts of speech for each word\n",
        "    #pos_tags = [(token.text, token.pos_) for token in doc]\n",
        "\n",
        "    filtered_words = [token.text for token in doc if token.pos_ in ['NOUN', 'VERB']]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "\n",
        "\n",
        "def customer_says(text):\n",
        "    #text = substring(text)\n",
        "    #text = re.sub(r'[^a-z\\s]', ' ', text)\n",
        "    #text = replace_consecutive_xs(text)\n",
        "    pattern = re.compile(r'customer says (.*?)(agent says|customer says|$)')\n",
        "    matches = re.findall(pattern, text)\n",
        "\n",
        "\n",
        "    captured_texts = [match[0] for match in matches if len(match[0].split())>4]\n",
        "\n",
        "    #captured_texts = [\n",
        "    #for s in matches:\n",
        "        #words = s[0].split()\n",
        "        #l = len(words)\n",
        "        #if l>4:\n",
        "            #s_filtered = POS_filter(s[0])\n",
        "            #captured_texts.append(s_filtered)\n",
        "        #else:\n",
        "            #continue\n",
        "\n",
        "\n",
        "    customer_said = ' '.join(captured_texts)\n",
        "\n",
        "    #filtered_text = remove_small_sentences(customer_said)\n",
        "    #filtered_text = POS_filter(filtered_text)\n",
        "\n",
        "    # Display the result\n",
        "    return POS_filter(customer_said)\n",
        "\n",
        "\n",
        "def agent_says(text):\n",
        "    #text = substring(text)\n",
        "    #text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    #text = replace_consecutive_xs(text)\n",
        "    pattern = re.compile(r'agent says (.*?)(agent says|customer says|$)')\n",
        "    matches = re.findall(pattern, text)\n",
        "\n",
        "\n",
        "    captured_texts = [match[0] for match in matches if len(match[0].split())>4]\n",
        "    agent_said = ' '.join(captured_texts)\n",
        "\n",
        "    # Display the result\n",
        "    return POS_filter(agent_said)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77ODkIjNLWAy",
        "outputId": "8a755819-9721-4982-cfef-ff3b7620c17d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bought size need return have do amazon bring store\n"
          ]
        }
      ],
      "source": [
        "print(customer_says(train_df['text'].iloc[209342]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhehS8tgLvHv",
        "outputId": "a23633a9-9247-46d2-b98b-8acda3b3990a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "thank calling name have pleasure speak date let put touch post purchase support give answer\n"
          ]
        }
      ],
      "source": [
        "print(agent_says(train_df['text'].iloc[209342]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oiKjc4hMI5Up",
        "outputId": "6c39047a-ebb3-4341-eba1-d9986345d9d0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8917887d-99a6-43c8-8dea-be02ee13a75e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>processed_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>agent says thanks for contacting geek squad. m...</td>\n",
              "      <td>product availability and stock</td>\n",
              "      <td>had question had question s have have question...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8917887d-99a6-43c8-8dea-be02ee13a75e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8917887d-99a6-43c8-8dea-be02ee13a75e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8917887d-99a6-43c8-8dea-be02ee13a75e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                text  \\\n",
              "0  agent says thanks for contacting geek squad. m...   \n",
              "\n",
              "                            label  \\\n",
              "0  product availability and stock   \n",
              "\n",
              "                                      processed_text  \n",
              "0  had question had question s have have question...  "
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df['processed_text'] = train_df['text'].apply(lambda x: customer_says(x)+ ' ' + agent_says(x))\n",
        "valid_df['processed_text'] = valid_df['text'].apply(lambda x: customer_says(x)+ ' ' + agent_says(x))\n",
        "test_df['processed_text'] = test_df['text'].apply(lambda x: customer_says(x)+ ' ' + agent_says(x))\n",
        "#test_df['clean_text'] = test_df['text'].apply(lambda x: agent_says(x))\n",
        "#test_df[['processed_text','label']].to_excel('test_processed.xlsx')\n",
        "train_df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqCONSDJJxnB"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "def get_most_common_pos(word):\n",
        "    synsets = wordnet.synsets(word)\n",
        "\n",
        "    if synsets:\n",
        "        pos_counts = {}\n",
        "        for synset in synsets:\n",
        "            pos = synset.pos()\n",
        "            pos_counts[pos] = pos_counts.get(pos, 0) + 1\n",
        "\n",
        "        most_common_pos = max(pos_counts, key=pos_counts.get)\n",
        "        return most_common_pos\n",
        "    else:\n",
        "        # If no synsets found, default to 'n' (noun)\n",
        "        return 'x'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "rGSW0VGFa9Ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQxf2CpLJxpZ"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove non-alpha characters and replace with space\n",
        "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    #pos_tags = pos_tag(words)\n",
        "\n",
        "\n",
        "\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Remove courtesy & non verbal words\n",
        "    courtesy_words = ['hi', 'hello','hey', 'thank','you','agent','customer','says','calling','uh','mhm','oh','i',\n",
        "                     'name','phone','number','phonenumber','okay','yeah','um','date','bye','contacting','please','yes','no']\n",
        "    words = [word for word in filtered_words if word not in courtesy_words]\n",
        "\n",
        "    # Stemming\n",
        "\n",
        "\n",
        "    #Lemmatization\n",
        "    #lemmatizer = WordNetLemmatizer()\n",
        "    #words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    #words = [word for word in words if get_most_common_pos(word) in ('n','v')]\n",
        "\n",
        "    stemmer = PorterStemmer()\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "    cleaned_text = ' '.join(words)\n",
        "\n",
        "    return cleaned_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffYaqHnLJxrs"
      },
      "outputs": [],
      "source": [
        "print(preprocess_text(train_df['processed_text'].iloc[10]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RPK1e5zJxuI"
      },
      "outputs": [],
      "source": [
        "valid_df['CleanedText'] = valid_df['processed_text'].apply(preprocess_text)\n",
        "test_df['CleanedText'] = test_df['processed_text'].apply(preprocess_text)\n",
        "train_df['CleanedText'] = train_df['processed_text'].apply(preprocess_text)\n",
        "print(train_df['CleanedText'].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bp8xyravJxwa"
      },
      "outputs": [],
      "source": [
        "train_df.dropna(subset=['CleanedText'], inplace=True)\n",
        "train_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtK9PDa3Jxyu"
      },
      "outputs": [],
      "source": [
        "#vectorizing and removing features that are super rare(<5 docs)\n",
        "#train_df['tokenized_text'] = train_df['CleanedText'].apply(lambda doc: word_tokenize(doc))\n",
        "\n",
        "vectorizer = CountVectorizer(binary=True)\n",
        "X = vectorizer.fit_transform(train_df['CleanedText'])\n",
        "vocabulary = vectorizer.get_feature_names_out()\n",
        "min_document_frequency = 5\n",
        "document_frequencies = (X > 0).sum(axis=0)\n",
        "\n",
        "selected_features = [idx for idx, freq in enumerate(document_frequencies.A[0]) if freq >= min_document_frequency]\n",
        "filtered_vocabulary = [vocabulary[idx] for idx in selected_features]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3m_nHbb5Jx0y"
      },
      "outputs": [],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(vocabulary=filtered_vocabulary)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(train_df['CleanedText'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_train_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "EjgDjn1pfTqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_classifier = MultinomialNB()\n",
        "\n",
        "# Train the classifier\n",
        "nb_classifier.fit(tfidf_train_df, train_df['label'])"
      ],
      "metadata": {
        "id": "_kGt_VROfTtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_tfidf_valid = TfidfVectorizer.transform(valid_df['CleanedText'])\n",
        "\n",
        "# Convert the TF-IDF matrix to a DataFrame for better readability\n",
        "tfidf_valid = pd.DataFrame(X_tfidf_valid.toarray(), columns=tfidf_vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "OXn-Vc1wfTvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = nb_classifier.predict(tfidf_valid)\n",
        "print(predictions[:10])"
      ],
      "metadata": {
        "id": "o_PYu2u0fTyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actual_labels = valid_df['label'].values\n",
        "print(actual_labels[:10])"
      ],
      "metadata": {
        "id": "G4OaTeMPfT07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy score is: \",accuracy_score(actual_labels, predictions)*100)"
      ],
      "metadata": {
        "id": "9TbygJP0fT3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score_val = f1_score(actual_labels, predictions, average='weighted')\n",
        "print(f1_score_val*100)"
      ],
      "metadata": {
        "id": "dPnjLMqqgRdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "metadata": {
        "id": "5y3Pco6CgRfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_params = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
        "svm_model = GridSearchCV(SVC(), svm_params, cv=3)\n",
        "svm_model.fit(tfidf_train_df, train_df['label'])\n",
        "best_model = svm_model.best_estimator_"
      ],
      "metadata": {
        "id": "0HTgWh2NgRiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_pred = svm_model.predict(tfidf_valid)\n",
        "svm_accuracy = accuracy_score(actual_labels, svm_pred)\n",
        "print(\"SVM Accuracy:\", svm_accuracy*100)\n",
        "svm_f1 = f1_score(actual_labels, svm_pred, average='weighted')\n",
        "print(svm_f1*100)"
      ],
      "metadata": {
        "id": "MZIbkUmFgRkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Trying out multiple models\n",
        "# Naive Bayes\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(tfidf_train_df, train_df['label'])\n",
        "\n",
        "# Random Forest\n",
        "rf_params = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20]}\n",
        "rf_model = GridSearchCV(RandomForestClassifier(), rf_params, cv=3)\n",
        "rf_model.fit(tfidf_train_df, train_df['label'])\n",
        "\n",
        "# Neural Network\n",
        "nn_params = {'hidden_layer_sizes': [(50, ), (100, ), (50, 50)], 'max_iter': [100, 200, 300]}\n",
        "nn_model = GridSearchCV(MLPClassifier(), nn_params, cv=3)\n",
        "nn_model.fit(tfidf_train_df, train_df['label'])\n",
        "\n",
        "# Make predictions on the test set\n",
        "\n",
        "nb_predictions = nb_model.predict(tfidf_valid)\n",
        "rf_predictions = rf_model.predict(tfidf_valid)\n",
        "nn_predictions = nn_model.predict(tfidf_valid)\n",
        "\n",
        "# Evaluate the models\n",
        "svm_accuracy = accuracy_score(actual_labels, svm_predictions)\n",
        "nb_accuracy = accuracy_score(actual_labels, nb_predictions)\n",
        "rf_accuracy = accuracy_score(actual_labels, rf_predictions)\n",
        "nn_accuracy = accuracy_score(actual_labels, nn_predictions)\n",
        "\n",
        "print(\"SVM Accuracy:\", svm_accuracy)\n",
        "print(\"Naive Bayes Accuracy:\", nb_accuracy)\n",
        "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
        "print(\"Neural Network Accuracy:\", nn_accuracy)\n",
        "\n",
        "\n",
        "svm_f1 = f1_score(actual_labels, svm_predictions, average='weighted')\n",
        "nb_f1 = f1_score(actual_labels, nb_predictions, average='weighted')\n",
        "rf_f1 = f1_score(actual_labels, rf_predictions, average='weighted')\n",
        "nn_f1 = f1_score(actual_labels, nn_predictions, average='weighted')\n",
        "\n",
        "# Print F1 scores\n",
        "print(\"SVM F1 Score:\", svm_f1)\n",
        "print(\"Naive Bayes F1 Score:\", nb_f1)\n",
        "print(\"Random Forest F1 Score:\", rf_f1)\n",
        "print(\"Neural Network F1 Score:\", nn_f1)"
      ],
      "metadata": {
        "id": "r6qpl6d7hsIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7eOs9jtVhsLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ckL6NJbEhsOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CK8QcA4lhsQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yjTjYeZdhsTB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}